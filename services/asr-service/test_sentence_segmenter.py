#!/usr/bin/env python3
"""
æ™ºèƒ½åˆ†å¥å™¨æµ‹è¯•è„šæœ¬
"""

import sys
import logging
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from sentence_segmenter import SentenceSegmenter

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_basic_segmentation():
    """æµ‹è¯• 1: åŸºæœ¬åˆ†å¥åŠŸèƒ½"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: åŸºæœ¬åˆ†å¥åŠŸèƒ½")
    print("="*60)

    segmenter = SentenceSegmenter()

    test_cases = [
        # æµ‹è¯•ç”¨ä¾‹ 1: æ ‡å‡†åˆ†å¥
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚æˆ‘å»å…¬å›­æ•£æ­¥äº†ã€‚çœ‹åˆ°å¾ˆå¤šäººåœ¨é”»ç‚¼ã€‚",

        # æµ‹è¯•ç”¨ä¾‹ 2: æ— æ ‡ç‚¹çš„é•¿å¥
        "æˆ‘åœ¨ä½¿ç”¨GitHubè¿›è¡Œç‰ˆæœ¬æ§åˆ¶ç„¶åä½¿ç”¨Dockeréƒ¨ç½²åº”ç”¨æœ€åä½¿ç”¨pytestè¿›è¡Œæµ‹è¯•",

        # æµ‹è¯•ç”¨ä¾‹ 3: æ··åˆæ ‡ç‚¹
        "è¿™æ˜¯ç¬¬ä¸€å¥ï¼ŒåŒ…å«é€—å·ï¼›è¿™æ˜¯ç¬¬äºŒå¥ï¼Œç”¨åˆ†å·åˆ†éš”ï¼è¿™æ˜¯ç¬¬ä¸‰å¥ï¼Ÿ",

        # æµ‹è¯•ç”¨ä¾‹ 4: æŠ€æœ¯æœ¯è¯­
        "æˆ‘ä»¬ä½¿ç”¨PyTorchè®­ç»ƒæ¨¡å‹ï¼Œç„¶åéƒ¨ç½²åˆ°Kubernetesé›†ç¾¤ï¼Œæœ€åé€šè¿‡APIæä¾›æœåŠ¡ã€‚",
    ]

    for i, text in enumerate(test_cases, 1):
        print(f"\nç”¨ä¾‹ {i}:")
        print(f"è¾“å…¥: {text}")

        sentences = segmenter.segment(text)

        print(f"è¾“å‡º ({len(sentences)} å¥):")
        for j, sent in enumerate(sentences, 1):
            print(f"  {j}. {sent}")

    print("\nâœ“ åŸºæœ¬åˆ†å¥æµ‹è¯•å®Œæˆ")


def test_long_sentence_splitting():
    """æµ‹è¯• 2: é•¿å¥æ‹†åˆ†"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: é•¿å¥æ‹†åˆ†")
    print("="*60)

    segmenter = SentenceSegmenter(
        max_sentence_length=50,
        split_long_sentences=True
    )

    # è¶…é•¿å¥å­ï¼ˆ150+ å­—ç¬¦ï¼‰
    long_text = (
        "åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­æˆ‘ä»¬ä½¿ç”¨å„ç§å·¥å…·å’ŒæŠ€æœ¯æ¥æé«˜å¼€å‘æ•ˆç‡"
        "æ¯”å¦‚ä½¿ç”¨Gitè¿›è¡Œç‰ˆæœ¬æ§åˆ¶ä½¿ç”¨Dockerè¿›è¡Œå®¹å™¨åŒ–éƒ¨ç½²"
        "ä½¿ç”¨Kubernetesè¿›è¡Œå®¹å™¨ç¼–æ’ä½¿ç”¨Jenkinsè¿›è¡ŒæŒç»­é›†æˆ"
        "ä½¿ç”¨Prometheusè¿›è¡Œç›‘æ§ä½¿ç”¨ELKè¿›è¡Œæ—¥å¿—åˆ†æ"
        "è¿™äº›å·¥å…·å¤§å¤§æé«˜äº†æˆ‘ä»¬çš„å·¥ä½œæ•ˆç‡"
    )

    print(f"è¾“å…¥ ({len(long_text)} å­—ç¬¦):")
    print(f"{long_text}")

    sentences = segmenter.segment(long_text)

    print(f"\nè¾“å‡º ({len(sentences)} å¥):")
    for i, sent in enumerate(sentences, 1):
        print(f"  {i}. [{len(sent)}å­—] {sent}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = segmenter.get_stats(sentences)
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»å¥æ•°: {stats['total_sentences']}")
    print(f"  å¹³å‡é•¿åº¦: {stats['avg_length']:.1f} å­—ç¬¦")
    print(f"  æœ€çŸ­å¥: {stats['min_length']} å­—ç¬¦")
    print(f"  æœ€é•¿å¥: {stats['max_length']} å­—ç¬¦")

    print("\nâœ“ é•¿å¥æ‹†åˆ†æµ‹è¯•å®Œæˆ")


def test_short_sentence_merging():
    """æµ‹è¯• 3: çŸ­å¥åˆå¹¶"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: çŸ­å¥åˆå¹¶")
    print("="*60)

    segmenter = SentenceSegmenter(
        min_sentence_length=15,
        merge_short_sentences=True
    )

    # å¤šä¸ªçŸ­å¥
    short_text = "æˆ‘å¾ˆé«˜å…´ã€‚å› ä¸ºä»Šå¤©æ˜¯å‘¨æœ«ã€‚å¯ä»¥ä¼‘æ¯ã€‚ä¸ç”¨ä¸Šç­ã€‚"

    print(f"è¾“å…¥:")
    print(f"{short_text}")

    sentences = segmenter.segment(short_text)

    print(f"\nè¾“å‡º ({len(sentences)} å¥):")
    for i, sent in enumerate(sentences, 1):
        print(f"  {i}. [{len(sent)}å­—] {sent}")

    print("\nâœ“ çŸ­å¥åˆå¹¶æµ‹è¯•å®Œæˆ")


def test_tech_terms_protection():
    """æµ‹è¯• 4: æŠ€æœ¯æœ¯è¯­ä¿æŠ¤"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: æŠ€æœ¯æœ¯è¯­ä¿æŠ¤")
    print("="*60)

    segmenter = SentenceSegmenter(
        protect_tech_terms=True
    )

    tech_text = (
        "æˆ‘ä»¬ä½¿ç”¨GitHubè¿›è¡Œç‰ˆæœ¬æ§åˆ¶ã€‚"
        "é¡¹ç›®åœ°å€æ˜¯https://github.com/user/repoã€‚"
        "ä½¿ç”¨çš„ç‰ˆæœ¬æ˜¯1.0.0ã€‚"
        "æ”¯æŒHTTPå’ŒHTTPSåè®®ã€‚"
        "ä½¿ç”¨PyTorchå’ŒTensorFlowè®­ç»ƒæ¨¡å‹ã€‚"
    )

    print(f"è¾“å…¥:")
    print(f"{tech_text}")

    sentences = segmenter.segment(tech_text)

    print(f"\nè¾“å‡º ({len(sentences)} å¥):")
    for i, sent in enumerate(sentences, 1):
        print(f"  {i}. {sent}")

    print("\nâœ“ æŠ€æœ¯æœ¯è¯­ä¿æŠ¤æµ‹è¯•å®Œæˆ")


def test_conjunction_handling():
    """æµ‹è¯• 5: è¿æ¥è¯å¤„ç†"""
    print("\n" + "="*60)
    print("æµ‹è¯• 5: è¿æ¥è¯å¤„ç†")
    print("="*60)

    segmenter = SentenceSegmenter(
        merge_short_sentences=True
    )

    conjunction_text = "æˆ‘æƒ³å»å…¬å›­ã€‚ä½†æ˜¯å¤©æ°”ä¸å¥½ã€‚æ‰€ä»¥æˆ‘å¾…åœ¨å®¶é‡Œã€‚"

    print(f"è¾“å…¥:")
    print(f"{conjunction_text}")

    sentences = segmenter.segment(conjunction_text)

    print(f"\nè¾“å‡º ({len(sentences)} å¥):")
    for i, sent in enumerate(sentences, 1):
        print(f"  {i}. {sent}")

    print("\nâœ“ è¿æ¥è¯å¤„ç†æµ‹è¯•å®Œæˆ")


def test_real_asr_output():
    """æµ‹è¯• 6: çœŸå® ASR è¾“å‡º"""
    print("\n" + "="*60)
    print("æµ‹è¯• 6: çœŸå® ASR è¾“å‡ºæ¨¡æ‹Ÿ")
    print("="*60)

    segmenter = SentenceSegmenter()

    # æ¨¡æ‹Ÿ ASR è¾“å‡ºï¼ˆå¯èƒ½æ²¡æœ‰æ ‡ç‚¹ï¼Œæˆ–æ ‡ç‚¹ä¸å‡†ç¡®ï¼‰
    asr_outputs = [
        "æˆ‘åœ¨å¼€å‘ä¸€ä¸ªäººå·¥æ™ºèƒ½é¡¹ç›®ä½¿ç”¨äº†PyTorchæ¡†æ¶å’ŒDockerå®¹å™¨åŒ–éƒ¨ç½²å¹¶ä¸”ä½¿ç”¨äº†GitHubè¿›è¡Œç‰ˆæœ¬ç®¡ç†",
        "ä»Šå¤©çš„ä¼šè®®è®¨è®ºäº†ä¸‰ä¸ªè®®é¢˜é¦–å…ˆæ˜¯é¡¹ç›®è¿›åº¦ç„¶åæ˜¯é¢„ç®—é—®é¢˜æœ€åæ˜¯äººå‘˜å®‰æ’å¤§å®¶éƒ½å¾ˆç§¯æå‘è¨€",
        "æ—©ä¸Šä¹ç‚¹å¼€å§‹å·¥ä½œå…ˆæ£€æŸ¥é‚®ä»¶ç„¶åå‚åŠ ç«™ä¼šæ¥ç€å¼€å§‹ç¼–ç ä¸­åˆåäºŒç‚¹åƒé¥­ä¸‹åˆç»§ç»­å·¥ä½œäº”ç‚¹ä¸‹ç­",
    ]

    for i, text in enumerate(asr_outputs, 1):
        print(f"\nASR è¾“å‡º {i} ({len(text)} å­—ç¬¦):")
        print(f"åŸæ–‡: {text}")

        sentences = segmenter.segment(text)

        print(f"åˆ†å¥å ({len(sentences)} å¥):")
        for j, sent in enumerate(sentences, 1):
            print(f"  {j}. {sent}")

        stats = segmenter.get_stats(sentences)
        print(f"  å¹³å‡å¥é•¿: {stats['avg_length']:.1f} å­—ç¬¦")

    print("\nâœ“ çœŸå® ASR è¾“å‡ºæµ‹è¯•å®Œæˆ")


def test_metadata():
    """æµ‹è¯• 7: å…ƒæ•°æ®åŠŸèƒ½"""
    print("\n" + "="*60)
    print("æµ‹è¯• 7: å…ƒæ•°æ®åŠŸèƒ½")
    print("="*60)

    segmenter = SentenceSegmenter()

    text = "è¿™æ˜¯ç¬¬ä¸€å¥ã€‚è¿™æ˜¯ç¬¬äºŒå¥ã€‚è¿™æ˜¯ç¬¬ä¸‰å¥ã€‚"

    print(f"è¾“å…¥: {text}")

    segments = segmenter.segment_with_metadata(text)

    print(f"\nè¾“å‡º ({len(segments)} ä¸ªç‰‡æ®µ):")
    for i, seg in enumerate(segments, 1):
        print(f"  {i}. [{seg.start_pos}:{seg.end_pos}] {seg.text}")

    print("\nâœ“ å…ƒæ•°æ®åŠŸèƒ½æµ‹è¯•å®Œæˆ")


def test_edge_cases():
    """æµ‹è¯• 8: è¾¹ç•Œæƒ…å†µ"""
    print("\n" + "="*60)
    print("æµ‹è¯• 8: è¾¹ç•Œæƒ…å†µ")
    print("="*60)

    segmenter = SentenceSegmenter()

    edge_cases = [
        ("", "ç©ºå­—ç¬¦ä¸²"),
        ("   ", "ä»…ç©ºæ ¼"),
        ("ä½ å¥½", "æçŸ­æ–‡æœ¬"),
        ("ã€‚ã€‚ã€‚", "ä»…æ ‡ç‚¹"),
        ("Hello World", "çº¯è‹±æ–‡"),
        ("123456", "çº¯æ•°å­—"),
        ("ä½ å¥½ï¼ï¼ï¼", "å¤šä¸ªç›¸åŒæ ‡ç‚¹"),
    ]

    for text, description in edge_cases:
        print(f"\n{description}: '{text}'")
        sentences = segmenter.segment(text)
        print(f"  ç»“æœ: {sentences}")

    print("\nâœ“ è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆ")


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸš€ " * 20)
    print("æ™ºèƒ½åˆ†å¥å™¨æµ‹è¯•")
    print("ğŸš€ " * 20)

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_basic_segmentation()
    test_long_sentence_splitting()
    test_short_sentence_merging()
    test_tech_terms_protection()
    test_conjunction_handling()
    test_real_asr_output()
    test_metadata()
    test_edge_cases()

    print("\n" + "="*60)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
